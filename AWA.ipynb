{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AWA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"izUHMlyzWXx1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597935131289,"user_tz":-270,"elapsed":2050,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}},"outputId":"f4341c5b-b79a-4e3c-c098-2f1e1499c36a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pzeOhIENEft1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":674},"executionInfo":{"status":"ok","timestamp":1597935143228,"user_tz":-270,"elapsed":13976,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}},"outputId":"c211de60-074c-45ef-a56c-f371099ac0de"},"source":["%cd /content/drive/My Drive/AWA/AWA_Final\n","\n","!nvcc --version\n","'''\n","!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n","!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n","!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n","!apt-get update\n","!apt-get install cuda=9.0.176-1\n","'''\n","%tensorflow_version 1.x\n","\n","\n","\n","#!pip install tensorflow-gpu==1.12.0\n","\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AWA/AWA_Final\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2019 NVIDIA Corporation\n","Built on Sun_Jul_28_19:07:16_PDT_2019\n","Cuda compilation tools, release 10.1, V10.1.243\n","TensorFlow 1.x selected.\n","1.15.2\n","[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 17415653005493388501\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 17470780873586467861\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 914299420489647391\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14912199066\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 5544916511478971503\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hl02ipq3ibel","colab_type":"text"},"source":["#Config"]},{"cell_type":"code","metadata":{"id":"7DJjRn2likRz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597935143229,"user_tz":-270,"elapsed":13973,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}}},"source":["AWA_type = 'UAWA'\n","tau_high = 0.30\n","tau_low = 0.05\n","OH = 0.50\n","exp_num = 1"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSoG5n1vipaZ","colab_type":"text"},"source":["#Load Dataset"]},{"cell_type":"code","metadata":{"id":"ZvbdNgJoiwCU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597935222146,"user_tz":-270,"elapsed":92887,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}}},"source":["import pickle\n","import numpy as np\n","import os\n","import random\n","TRACE_LENGTH = 2000\n","num_sampels_in_Gs_training = 400\n","DF_NB_CLASSES = 95\n","def dir_to_burst(dir_data):\n","    burst_data = np.zeros(dir_data.shape)\n","    for i in range(len(dir_data)):\n","        k = 0\n","        dir_sign = dir_data[i][0]\n","        for j in range(TRACE_LENGTH):\n","            if dir_data[i][j] == 0:\n","                break\n","            if dir_data[i][j] != dir_sign:\n","                k += 1\n","                dir_sign = dir_data[i][j]\n","            burst_data[i][k] += dir_data[i][j]\n","    return burst_data\n","\n","\n","def LoadDataNoDefCW():\n","\n","    print(\"Loading non-defended dataset for closed-world scenario\")\n","    # Point to the directory storing data\n","\n","\n","    # X represents a sequence of traffic directions\n","    # y represents a sequence of corresponding label (website's label)\n","\n","    # Load training data\n","    with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n","        X_train = np.array(pickle.load(handle, encoding=\"latin1\"))\n","    with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n","        y_train = np.array(pickle.load(handle, encoding=\"latin1\"))\n","\n","    # Load validation data\n","    with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n","        X_valid = np.array(pickle.load(handle, encoding=\"latin1\"))\n","    with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n","        y_valid = np.array(pickle.load(handle, encoding=\"latin1\"))\n","\n","    # Load testing data\n","    with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n","        X_test = np.array(pickle.load(handle, encoding=\"latin1\"))\n","    with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n","        y_test = np.array(pickle.load(handle, encoding=\"latin1\"))\n","    \n","    y_valid = np.eye(DF_NB_CLASSES)[y_valid]\n","    y_train = np.eye(DF_NB_CLASSES)[y_train]\n","    y_test = np.eye(DF_NB_CLASSES)[y_test]\n","\n","    X_train = X_train[:, :, np.newaxis]\n","    X_valid = X_valid[:, :, np.newaxis]\n","    X_test = X_test[:, :, np.newaxis]\n","\n","    print(\"Data dimensions:\")\n","    print(\"X: Training data's shape : \", X_train.shape)\n","    print(\"y: Training data's shape : \", y_train.shape)\n","    print(\"X: Validation data's shape : \", X_valid.shape)\n","    print(\"y: Validation data's shape : \", y_valid.shape)\n","    print(\"X: Testing data's shape : \", X_test.shape)\n","    print(\"y: Testing data's shape : \", y_test.shape)\n","\n","    return dir_to_burst(X_train), y_train, dir_to_burst(X_valid), y_valid, dir_to_burst(X_test), y_test\n","\n","class DFDataLoader():\n","    \"\"\"An image loader that uses just a few ImageNet-like images.\n","    In the actual paper, we used real ImageNet images, but we can't include them\n","    here because of licensing issues.\n","    \"\"\"\n","\n","    def __init__(self,nb_classes_df,load_burst_data=1):\n","        if load_burst_data == 0:\n","            self.train_data,self.train_labels,self.validation_data,self.validation_labels,self.test_data,self.test_labels = LoadDataNoDefCW()\n","            np.savez('burst_data.npz', train_x=self.train_data, train_y=self.train_labels, val_x = self.validation_data, val_y = self.validation_labels, test_x = self.test_data, test_y = self.test_labels)\n","        else:\n","            burst_data = np.load('burst_data.npz')\n","            train_data = data=burst_data['train_x'][:,:TRACE_LENGTH] \n","            train_labels = burst_data['train_y']\n","            self.validation_data = data=burst_data['val_x'][:,:TRACE_LENGTH] \n","            self.validation_labels = burst_data['val_y']\n","            self.test_data = data=burst_data['test_x'][:,:TRACE_LENGTH] \n","            self.test_labels = burst_data['test_y']\n","            transformer_train_x = []\n","            transformer_train_y = []\n","            user_train_x = []\n","            user_train_y = []\n","            for i in range(nb_classes_df):\n","                data_x = train_data[np.argmax(train_labels,axis=1) == i]\n","                data_y = train_labels[np.argmax(train_labels,axis=1) == i]\n","                transformer_train_x.append(data_x[:400])\n","                transformer_train_y.append(data_y[:400])\n","                user_train_x.append(data_x[400:])\n","                user_train_y.append(data_y[400:])\n","            self.transformer_train_x = np.array(transformer_train_x).reshape(nb_classes_df*400,TRACE_LENGTH,1)\n","            self.transformer_train_y = np.array(transformer_train_y).reshape(nb_classes_df*400,nb_classes_df)\n","            self.user_train_x = np.array(user_train_x).reshape(nb_classes_df*400,TRACE_LENGTH,1)\n","            self.user_train_y = np.array(user_train_y).reshape(nb_classes_df*400,nb_classes_df)\n","        self.nb_classes_df = nb_classes_df\n","\n","\n","    def get_DF_data(self,data_type,batch_size=None,target_cls=None,src_cls=None):\n","        assert bool(target_cls != None) != bool(src_cls != None), \"BAD func call\"\n","        if data_type == 'transformer_train':\n","            data = self.transformer_train_x\n","            label = self.transformer_train_y\n","        elif data_type == 'user_train':\n","            data = self.user_train_x\n","            label = self.user_train_y\n","        elif data_type == 'validation':\n","            data = self.validation_data\n","            label = self.validation_labels\n","        elif data_type == 'test':\n","            data = self.test_data\n","            label = self.test_labels\n","        else:\n","            print('Please set the data_type variable')\n","        if target_cls != None:\n","            other_class_data = data[np.argmax(label,axis=1) != target_cls]\n","            other_class_labels = label[np.argmax(label,axis=1) != target_cls]\n","            if batch_size != None:\n","                sample = random.sample(list(np.arange(other_class_data.shape[0])), batch_size)\n","                return other_class_data[sample],other_class_labels[sample]\n","            return other_class_data,other_class_labels\n","        if src_cls != None:\n","            src_class_data = data[np.argmax(label,axis=1) == src_cls]\n","            src_class_labels = label[np.argmax(label,axis=1) == src_cls]\n","            if batch_size != None:\n","                sample = random.sample(list(np.arange(src_class_data.shape[0])), batch_size)\n","                return src_class_data[sample],src_class_labels[sample]\n","            return src_class_data,src_class_labels\n","\n","class AT_Dataset():\n","    def __init__(self):\n","        self.train_data = 0\n","        self.train_labels = 0\n","        self.validation_data = 0\n","        self.validation_labels = 0\n","        self.test_data = 0\n","        self.test_labels = 0\n","\n","Trace_loader = DFDataLoader(DF_NB_CLASSES)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5dNTHX_iw7m","colab_type":"text"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"VTfswIpni4xW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597935222559,"user_tz":-270,"elapsed":93296,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}}},"source":["import os\n","import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","import tensorflow as tf\n","from  tensorflow.python.keras.layers import Input, Conv1D, Activation, BatchNormalization, Lambda, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Dense, ELU, MaxPooling1D, Reshape, Average, ReLU, Lambda\n","from tensorflow.python.keras.initializers import glorot_uniform\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","from tensorflow.python.data import Dataset\n","\n","\n","NB_CLASSES = 95 # number of outputs = number of classes\n","\n","\n","\n","def loss_plot(g1_l,g2_l,d1_l,d2_l=None,path=None, ind = None):\n","    x = np.arange(len(g1_l))\n","    plt.xlabel(\"Step\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Loss graph\")\n","    plt.plot(x,np.array(g1_l),label = \"G1_Loss\")\n","    plt.plot(x,g2_l,label = \"G2_Loss\")\n","    if d1_l != None:\n","        plt.plot(x,d1_l,label = \"D_Loss\")\n","    if d2_l != None:\n","        plt.plot(x,d2_l,label = \"D2_Loss\")\n","    plt.legend()\n","    if path == None:\n","        plt.show()\n","    else:\n","        plt.savefig(path+'loss_'+str(ind))\n","    plt.close()\n","\n","import PIL.Image\n","def visualize(trace_len,src=None,trg=None,g1=None,g2=None,path=None,ind=None):\n","    src = np.round(np.sum(src,axis=0) / len(src)).reshape(1,trace_len,1)\n","    trg = np.round(np.sum(trg,axis=0) / len(trg)).reshape(1,trace_len,1)\n","    g1 = np.round(np.sum(g1,axis=0) / len(g1)).reshape(1,trace_len,1)\n","    g2 = np.round(np.sum(g2,axis=0) / len(g2)).reshape(1,trace_len,1)\n","    plt.plot(src[0],'.', color='blue')\n","    plt.plot(trg[0],'.', color='red')\n","    if True:\n","        plt.plot(g1[0],'.', color='red')\n","        plt.plot(g2[0],'.', color='blue')\n","    plt.plot(np.zeros(src.shape)[0],'.', color='black')\n","    plt.ylabel('value')\n","    plt.xlabel('ind')\n","    if True:\n","        plt.legend(['src', 'trg','g1','g2'], loc='lower right')\n","\n"," #   plt.legend([str(c1),str(c2),'zero'], loc='lower right')\n","    if path == None:\n","        plt.show()\n","    else:\n","        plt.savefig(path+ 'vis_'+str(ind))\n","    plt.close()\n","    \n","def my_sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def print_overhead(name=\"\",orginal_data=None,manipulated_data=None, pr = 0):\n","    oh = np.round((np.sum(np.abs(manipulated_data) - np.abs(orginal_data)) / np.sum(np.abs(orginal_data))) * 100,2)\n","    if pr == 1:\n","        print(name,\"overhead\",oh )\n","    return oh\n","\n","def oh_acc_plot(oh1,oh2,oh3,oh4,acc_list,path=None, ind = None):\n","    x_oh = np.array(oh1)[:,1]\n","    if len(acc_list) > 0:\n","        x_acc = np.array(acc_list)[:,1]\n","    plt.xlabel(\"Step\")\n","    plt.title(\"Overhead vs Accuracy under 70%\")\n","    plt.plot(x_oh,np.array(oh1)[:,0],label = \"G1_OH_train\")\n","    plt.plot(x_oh,np.array(oh2)[:,0],label = \"G1_OH_test\")\n","    plt.plot(x_oh,np.array(oh3)[:,0],label = \"G2_OH_train\")\n","    plt.plot(x_oh,np.array(oh4)[:,0],label = \"G2_OH_test\")\n","    plt.plot(x_oh,np.ones(len(x_oh))*20,label = \"Baseline\",color='red')\n","    if len(acc_list) > 0:\n","        plt.plot(x_acc,np.array(acc_list)[:,0],label = \"Acc < 70%\", marker='o')\n","    plt.legend()\n","    if path == None:\n","        plt.show()\n","    else:\n","        plt.savefig(path+ 'OH_Acc_'+str(ind))\n","    plt.close()\n","\n","class ct1d():\n","    @staticmethod\n","    def Conv1DTranspose(model, filters=16, kernel_size=3, strides=2, padding='same'):\n","        model.add(Lambda(lambda x: tf.expand_dims(x, axis=2)))\n","        model.add(Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding))\n","        model.add(Lambda(lambda x: tf.squeeze(x, axis=2)))\n","\n","ct = ct1d()\n","\n","class data_class():\n","        \n","    def set_information(self,src_cls,trg_cls,oh_src_train,oh_trg_train):\n","        self.src_cls = src_cls\n","        self.trg_cls = trg_cls\n","\n","        self.oh_src_train = oh_src_train\n","        self.oh_trg_train = oh_trg_train\n","\n","\n","    def set_data(self,clean_src_trans_train,g_src_trans_train,clean_trg_trans_train,g_trg_trans_train,clean_src_user_train,g_src_user_train,clean_trg_user_train,g_trg_user_train,\n","                 clean_src_valid,g_src_valid,clean_trg_valid,g_trg_valid,clean_src_test,g_src_test,clean_trg_test,g_trg_test):\n","        self.clean_src_trans_train = clean_src_trans_train\n","        self.g_src_trans_train = g_src_trans_train\n","        self.clean_trg_trans_train = clean_trg_trans_train\n","        self.g_trg_trans_train = g_trg_trans_train\n","\n","        self.clean_src_user_train = clean_src_user_train\n","        self.g_src_user_train = g_src_user_train\n","        self.clean_trg_user_train = clean_trg_user_train\n","        self.g_trg_user_train = g_trg_user_train\n","\n","        self.clean_src_valid = clean_src_valid\n","        self.g_src_valid = g_src_valid\n","        self.clean_trg_valid = clean_trg_valid\n","        self.g_trg_valid = g_trg_valid\n","\n","        self.clean_src_test = clean_src_test\n","        self.g_src_test = g_src_test\n","        self.clean_trg_test = clean_trg_test\n","        self.g_trg_test = g_trg_test"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4nru0yEii7vu","colab_type":"text"},"source":["#Auxiliary Classifier Training"]},{"cell_type":"code","metadata":{"id":"GmruSuCtjK_e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597935241260,"user_tz":-270,"elapsed":111991,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}},"outputId":"d65fae53-8293-4e57-87c0-5cf96ff90dd7"},"source":["from tensorflow.python.keras.optimizers import Adamax\n","batch_size = 128\n","def AC_train(data, file_name, input_shape, classes, num_epochs=50, batch_size=128, train_temp=1, init=None,test_flag = 0,verbose=0,plot_flag=0,load_flag = 1,model_name=None):\n","\n","        \n","    model = tf.keras.Sequential()\n","\n","    filter_num = ['None',32,32,64,64]\n","    kernel_size = ['None',8,8,8,8]\n","    conv_stride_size = ['None',1,1,1,1]\n","    pool_stride_size = ['None',4,4,4,4]\n","    pool_size = ['None',8,8,8,8]\n","\n","    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=input_shape,\n","                         strides=conv_stride_size[1], padding='same',\n","                         name='ACblock1_conv1'))\n","    model.add(ELU(alpha=1.0, name='ACblock1_adv_act1'))\n","\n","    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n","                         strides=conv_stride_size[2], padding='same',\n","                         name='ACblock2_conv1'))\n","    model.add(ELU(alpha=1.0, name='ACblock1_adv_act2'))\n","    model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n","                               padding='same', name='ACblock1_pool'))\n","\n","    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n","                         strides=conv_stride_size[3], padding='same',\n","                         name='ACblock3_conv1'))\n","    model.add(ELU(alpha=1.0, name='ACblock2_adv_act1'))\n","\n","\n","    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n","                         strides=conv_stride_size[4], padding='same',\n","                         name='ACblock4_conv1'))\n","    model.add(ELU(alpha=1.0, name='ACblock2_adv_act2'))\n","    model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n","                               padding='same', name='ACblock2_pool'))\n","\n","    model.add(Flatten(name='ACflatten'))\n","    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='ACfc1'))\n","    model.add(Activation('relu', name='ACfc1_act'))\n","    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='ACfc2'))\n","    model.add(Activation('relu', name='ACfc2_act'))\n","    model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='ACfc3'))\n","    model.add(Activation('softmax', name=\"softmax\"))\n","\n","    if init != None:\n","        model.load_weights(init)\n","\n","    def fn(correct, predicted):\n","        return tf.nn.softmax_cross_entropy_with_logits(labels=correct,\n","                                                       logits=predicted / train_temp)\n","\n","    OPTIMIZER = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.9, beta2=0.999, epsilon=1e-08) # Optimizer\n","\n","    best_model_save = ModelCheckpoint('./'+model_name+'.hdf5', monitor='val_loss', verbose=0, save_best_only=True,\n","                                          save_weights_only=True, mode='auto', period=1)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,metrics=[\"accuracy\"])\n","    \n","\n","    if load_flag == 1:\n","        print(\"Loading AC Model!!!\")\n","        model.load_weights(model_name+'.hdf5')\n","        if test_flag == 1:\n","            score_test = model.evaluate(data.test_data, data.test_labels, verbose=verbose)\n","            y_pred = np.argmax(model.predict(data.test_data), axis=1)\n","            print('Confusion Matrix')\n","            print(confusion_matrix(np.argmax(data.test_labels, axis=1), y_pred))\n","            print(\"Testing accuracy:\", score_test[1])\n","            print(classification_report(np.argmax(data.test_labels, axis=1), y_pred))\n","        return model\n","\n","    print(\"train:\",data.transformer_train_x.shape,\"valid:\",data.transformer_train_x.shape,\"test:\",data.test_data.shape)\n","    if test_flag == 1:\n","        history = model.fit(data.transformer_train_x, data.transformer_train_y,batch_size=batch_size, epochs=num_epochs,verbose=verbose, validation_data=(data.transformer_train_x, data.transformer_train_y), callbacks=[best_model_save])\n","    else:\n","        history = model.fit(data.transformer_train_x, data.transformer_train_y,batch_size=batch_size, epochs=num_epochs,verbose=verbose, validation_data=(data.transformer_train_x, data.transformer_train_y))\n","    \n","    if test_flag == 1:\n","        print(\"Load Best Model\")\n","        model.load_weights(model_name+'.hdf5')\n","\n","        # Start evaluating model with testing data\n","        score_test = model.evaluate(data.test_data, data.test_labels, verbose=verbose)\n","        y_pred = np.argmax(model.predict(data.test_data), axis=1)\n","        print('Confusion Matrix')\n","        print(confusion_matrix(np.argmax(data.test_labels, axis=1), y_pred))\n","        print(\"Testing accuracy:\", score_test[1])\n","        print(classification_report(np.argmax(data.test_labels, axis=1), y_pred))\n","\n","    if file_name != None:\n","        model.save(file_name)\n","    \n","    return model\n","\n","AC = AC_train(Trace_loader,file_name=None,input_shape=(TRACE_LENGTH,1), classes=DF_NB_CLASSES, num_epochs=30,test_flag = 1,verbose=2,plot_flag=1,load_flag = 1,model_name='AC')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n","Loading AC Model!!!\n","9500/9500 - 8s - loss: 0.3897 - acc: 0.9407\n","Confusion Matrix\n","[[97  0  0 ...  0  0  0]\n"," [ 1 94  1 ...  0  0  0]\n"," [ 0  0 94 ...  0  0  0]\n"," ...\n"," [ 0  0  0 ... 95  0  0]\n"," [ 0  0  0 ...  0 92  0]\n"," [ 0  0  1 ...  0  0 90]]\n","Testing accuracy: 0.94073683\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.97      0.96       100\n","           1       0.98      0.94      0.96       100\n","           2       0.89      0.94      0.91       100\n","           3       0.97      0.98      0.98       100\n","           4       0.97      0.98      0.98       100\n","           5       0.95      0.98      0.97       100\n","           6       0.84      0.87      0.85       100\n","           7       0.98      0.99      0.99       100\n","           8       0.99      0.98      0.98       100\n","           9       0.94      0.92      0.93       100\n","          10       0.94      0.94      0.94       100\n","          11       0.91      0.95      0.93       100\n","          12       0.96      0.99      0.98       100\n","          13       0.92      0.79      0.85       100\n","          14       0.96      0.99      0.98       100\n","          15       1.00      0.98      0.99       100\n","          16       0.98      0.99      0.99       100\n","          17       0.97      0.94      0.95       100\n","          18       0.92      0.94      0.93       100\n","          19       0.99      1.00      1.00       100\n","          20       0.89      0.93      0.91       100\n","          21       0.97      0.96      0.96       100\n","          22       0.98      0.99      0.99       100\n","          23       0.97      0.94      0.95       100\n","          24       0.97      0.97      0.97       100\n","          25       0.99      0.96      0.97       100\n","          26       0.98      0.96      0.97       100\n","          27       0.89      0.94      0.91       100\n","          28       0.96      0.99      0.98       100\n","          29       0.96      0.95      0.95       100\n","          30       0.92      0.96      0.94       100\n","          31       0.97      0.97      0.97       100\n","          32       1.00      0.97      0.98       100\n","          33       0.92      0.94      0.93       100\n","          34       0.87      0.88      0.88       100\n","          35       0.81      0.92      0.86       100\n","          36       0.93      0.92      0.92       100\n","          37       0.98      0.90      0.94       100\n","          38       0.95      0.92      0.93       100\n","          39       0.85      0.98      0.91       100\n","          40       0.98      0.91      0.94       100\n","          41       0.99      0.97      0.98       100\n","          42       0.95      0.97      0.96       100\n","          43       0.98      0.98      0.98       100\n","          44       0.90      0.99      0.94       100\n","          45       0.94      0.91      0.92       100\n","          46       0.97      0.90      0.93       100\n","          47       0.90      0.92      0.91       100\n","          48       0.99      0.98      0.98       100\n","          49       0.84      0.81      0.83       100\n","          50       0.98      0.95      0.96       100\n","          51       0.97      0.98      0.98       100\n","          52       0.99      0.99      0.99       100\n","          53       0.89      0.92      0.91       100\n","          54       0.99      0.97      0.98       100\n","          55       0.96      0.92      0.94       100\n","          56       0.94      0.91      0.92       100\n","          57       1.00      0.93      0.96       100\n","          58       0.91      0.79      0.84       100\n","          59       0.93      0.95      0.94       100\n","          60       1.00      0.88      0.94       100\n","          61       0.95      0.90      0.92       100\n","          62       0.86      0.81      0.84       100\n","          63       0.94      0.95      0.95       100\n","          64       0.83      0.87      0.85       100\n","          65       0.90      0.99      0.94       100\n","          66       0.96      0.94      0.95       100\n","          67       0.98      0.86      0.91       100\n","          68       0.96      0.94      0.95       100\n","          69       0.95      1.00      0.98       100\n","          70       0.96      0.93      0.94       100\n","          71       0.96      0.92      0.94       100\n","          72       0.84      0.94      0.89       100\n","          73       0.91      0.96      0.93       100\n","          74       0.96      0.99      0.98       100\n","          75       0.97      0.97      0.97       100\n","          76       0.94      0.89      0.91       100\n","          77       0.92      0.98      0.95       100\n","          78       0.95      0.91      0.93       100\n","          79       0.96      0.96      0.96       100\n","          80       0.85      0.92      0.88       100\n","          81       0.94      0.97      0.96       100\n","          82       0.87      0.90      0.89       100\n","          83       0.98      0.93      0.95       100\n","          84       0.95      0.97      0.96       100\n","          85       0.92      0.98      0.95       100\n","          86       0.96      0.94      0.95       100\n","          87       0.93      0.95      0.94       100\n","          88       0.97      0.98      0.98       100\n","          89       0.96      0.95      0.95       100\n","          90       0.96      0.99      0.98       100\n","          91       0.97      0.87      0.92       100\n","          92       0.95      0.95      0.95       100\n","          93       0.97      0.92      0.94       100\n","          94       0.86      0.90      0.88       100\n","\n","    accuracy                           0.94      9500\n","   macro avg       0.94      0.94      0.94      9500\n","weighted avg       0.94      0.94      0.94      9500\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TXl9grj9jLqD","colab_type":"text"},"source":["#AWA Framework"]},{"cell_type":"code","metadata":{"id":"GMmDLLZHjaDK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597935241790,"user_tz":-270,"elapsed":112516,"user":{"displayName":"colab2 ams","photoUrl":"","userId":"12076015643111685554"}}},"source":["def AC_layers(layer_names):\n","    \"\"\" Creates a AC model that returns a list of intermediate output values.\"\"\"\n","    # Load our model. Load pretrained AC, trained on WF data\n","    AC = AC_train(Trace_loader,file_name=None,input_shape=(TRACE_LENGTH,1), classes=DF_NB_CLASSES, num_epochs=30,test_flag = 0,verbose=2,plot_flag=1,load_flag = 1,model_name='AC')\n","    AC.trainable = False\n","  \n","    outputs = [AC.get_layer(name).output for name in layer_names]\n","\n","    model = tf.keras.Model([AC.input], outputs)\n","    return model\n","\t\n","\t\n","\n","\n","class LogitModel(tf.keras.models.Model):\n","  def __init__(self, logit_layer):\n","    super(LogitModel, self).__init__()\n","    self.AC =  AC_layers(logit_layer)\n","    self.logit_layer = logit_layer\n","    self.AC.trainable = False\n","\n","  def call(self, inputs):\n","    outputs = self.AC(inputs)\n","    return outputs\n","\n","def cal_logit_loss(logit_outputs,src=None):\n","    logit_loss = logit_weight * tf.reduce_mean(tf.maximum(logit_outputs[:,src],0))\n","    return logit_loss\n","\t\n","class AWA_Class:\n","    def __init__(self, trace_len=None,logit_layer=None):\n","        self.session = tf.InteractiveSession()\n","        self.max_burst_trace_len = trace_len\n","        sign_vector = np.ones([trace_len,1]) * -1\n","        sign_vector[::2] = 1\n","        self.sign_vector = tf.convert_to_tensor(sign_vector, dtype=tf.float64)\n","        self.max_trace_len = tf.convert_to_tensor(trace_len, dtype=tf.float64)\n","\n","\n","        with tf.name_scope('placeholders'):\n","            self.cls_i_data = tf.placeholder(tf.float32, [None, self.max_burst_trace_len, 1], name=\"x_class_i\")\n","            self.cls_j_data = tf.placeholder(tf.float32, [None, self.max_burst_trace_len, 1], name=\"x_class_j\")\n","            self.noise_i = tf.placeholder(tf.float32, [None, self.max_burst_trace_len, 1], name=\"noise_i\")\n","            self.noise_j = tf.placeholder(tf.float32, [None, self.max_burst_trace_len, 1], name=\"noise_j\")\n","            self.src_1 = tf.placeholder(tf.int32, shape=(), name= \"src_1\")\n","            self.src_2 = tf.placeholder(tf.int32, shape=(), name= \"src_2\")\n","\n","        self.logit_extractor = LogitModel(logit_layer)\n","\n","        self.generator1 = self.make_generator1()\n","        self.generator2 = self.make_generator2()\n","        self.discriminator = self.make_discriminator()\n","\n","        if AWA_type == 'UAWA':\n","            self.generated_1 = self.generator1(self.noise_i)\n","            self.generated_2 = self.generator2(self.noise_j)\n","        elif AWA_type == 'NUAWA':\n","            self.generated_1 = self.generator1(self.cls_i_data)\n","            self.generated_2 = self.generator2(self.cls_j_data)          \n","        self.adjusted_generated_1 = self.adjust_WF_data(self.cls_i_data,self.generated_1)\n","        self.adjusted_generated_2 = self.adjust_WF_data(self.cls_j_data,self.generated_2)\n","        self.d_class_i = self.discriminator(self.adjusted_generated_1)\n","        self.d_class_j = self.discriminator(self.adjusted_generated_2)\n","        self.d_loss = self.discriminator_loss(self.d_class_i, self.d_class_j)\n","\n","        self.padded_adjusted_generated_1 = tf.pad(self.adjusted_generated_1,tf.constant([[0,0], [0, TRACE_LENGTH-self.max_burst_trace_len], [0,0]]), mode='CONSTANT', constant_values=0 )\n","        self.logit_outputs_1 = self.logit_extractor(self.padded_adjusted_generated_1)\n","        self.logit_loss_1 = cal_logit_loss(self.logit_outputs_1,src=self.src_1)\n","        self.g1_loss_org , self.g1_oh_loss = self.generator1_loss(self.adjusted_generated_1,self.cls_i_data,self.d_class_i)\n","        self.g1_loss = self.g1_loss_org + self.g1_oh_loss + self.logit_loss_1\n","\n","\n","        self.padded_adjusted_generated_2 = tf.pad(self.adjusted_generated_2,tf.constant([[0,0], [0, TRACE_LENGTH-self.max_burst_trace_len], [0,0]]), mode='CONSTANT', constant_values=0 )\n","        self.logit_outputs_2 = self.logit_extractor(self.padded_adjusted_generated_2)\n","        self.logit_loss_2 = cal_logit_loss(self.logit_outputs_2,src=self.src_2) \n","        self.g2_loss_org , self.g2_oh_loss = self.generator2_loss(self.adjusted_generated_2,self.cls_j_data,self.d_class_j)\n","        self.g2_loss = self.g2_loss_org + self.g2_oh_loss  + self.logit_loss_2\n","\n","        with tf.name_scope('optimizer'):\n","            optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n","            g1_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator1')\n","            self.g1_train = optimizer.minimize(self.g1_loss, var_list=g1_vars)\n","            g2_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator2')\n","            self.g2_train = optimizer.minimize(self.g2_loss, var_list=g2_vars)\n","            d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n","            self.d_train = optimizer.minimize(self.d_loss, var_list=d_vars)\n","        \n","        tf.global_variables_initializer().run()\n","\n","\n","  \n","    def adjust_WF_data(self,x = None,perturbation = None,sign_vector = None):\n","        mask = tf.expand_dims(tf.tile(tf.cast(tf.minimum(tf.sign(x[:,0]),0) * -1,tf.float32),multiples=[1,self.max_burst_trace_len]),2)   \n","        pert_rolled = tf.roll(tf.pad(perturbation,[[0,0],[0,1],[0,0]]),shift=1,axis=1)[:,:self.max_burst_trace_len] \n","        return x + ( (mask * pert_rolled + (1 - mask) * perturbation) * tf.sign(x) ) \n","\n","    def discriminator_loss(self, d_class_i, d_class_j):\n","        d_loss_i = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_class_i,labels=tf.ones_like(d_class_i)))\n","        d_loss_j = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_class_j,labels=tf.zeros_like(d_class_j)))\n","        d_loss = d_loss_i + d_loss_j\n","        return d_loss\n","\n","    def generator1_loss(self,perturbation_i=None,x_class_i=None,d_class_i=None):\n","        g1_oh_loss = tf.maximum(tf.reduce_mean(tf.math.divide(tf.reduce_sum(tf.abs(x_class_i - perturbation_i)),tf.reduce_sum(tf.abs(x_class_i)))) - tau_high, 0) \n","        g1_oh_loss_new = tf.minimum(tf.reduce_mean(tf.math.divide(tf.reduce_sum(tf.abs(x_class_i - perturbation_i)),tf.reduce_sum(tf.abs(x_class_i)))) - tau_low, 0) * -1\n","        g1_loss_org = -1 * (tf.reduce_mean(1 / 2 * tf.math.log(tf.math.sigmoid(d_class_i) + 0.0000001)) + tf.reduce_mean(1 / 2 * tf.math.log(1 - tf.math.sigmoid(d_class_i) + 0.0000001)))\n","        return disc_weight * g1_loss_org , oh_weight * (g1_oh_loss + g1_oh_loss_new)\n","\n","    def generator2_loss(self,perturbation_j=None,x_class_j=None,d_class_j=None):\n","        g2_oh_loss = tf.maximum(tf.reduce_mean(tf.math.divide(tf.reduce_sum(tf.abs(x_class_j - perturbation_j)),tf.reduce_sum(tf.abs(x_class_j)))) - tau_high, 0) \n","        g2_oh_loss_new = tf.minimum(tf.reduce_mean(tf.math.divide(tf.reduce_sum(tf.abs(x_class_j - perturbation_j)),tf.reduce_sum(tf.abs(x_class_j)))) - tau_low, 0) * -1\n","        g2_loss_org = -1 * (tf.reduce_mean(1 / 2 * tf.math.log(tf.math.sigmoid(d_class_j) + 0.0000001)) + tf.reduce_mean(1 / 2 * tf.math.log(1 - tf.math.sigmoid(d_class_j) + 0.0000001)))\n","        return disc_weight * g2_loss_org , oh_weight * (g2_oh_loss + g2_oh_loss_new)\n","\n","    def make_generator1(self):\n","        with tf.variable_scope('generator1'):\n","            model = tf.keras.Sequential()\n","            # ---------------------------------------------------------------------\n","            # c3s1-8\n","            model.add(Conv1D(filters=8, kernel_size=3, strides=1, padding='same',input_shape=(self.max_burst_trace_len,1)))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","            # -----------------------------------------------------------------\n","            # d16\n","            model.add(Conv1D(filters=16, kernel_size=3, strides=2, padding='same'))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # -------------------------------------------------------------------\n","            # d32\n","            model.add(Conv1D(filters=32, kernel_size=3, strides=2, padding='same'))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # ---------------------------------------------------------------------\n","            # four r32 blocks\n","            for _ in range(8):\n","                model.add(Conv1D(filters=32, kernel_size=3, strides=1, padding='same'))\n","                model.add(BatchNormalization(momentum=0.8))\n","                model.add(ELU(alpha=2.0))\n","    \n","            # -----------------------------------------------------------------------\n","            # u16\n","            ct.Conv1DTranspose(model, filters=16, kernel_size=3, strides=2, padding='same')\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","            # this below line is for the cases which our input shape is odd !!\n","            # G = Conv1D(filters=16, kernel_size=2, strides=1, padding='valid')(G)\n","    \n","            # ----------------------------------------------------------------------------\n","            # u8\n","            ct.Conv1DTranspose(model, filters=8, kernel_size=3, strides=2, padding='same')\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # -----------------------------------------------------------------------------\n","            # c3s1-3\n","            model.add(Conv1D(filters=1, kernel_size=3, strides=1, padding='same'))\n","            model.add(Activation('relu'))\n","            return model\n","    \n","    \n","    def make_generator2(self):\n","        with tf.variable_scope('generator2'):\n","            model = tf.keras.Sequential()\n","            # ---------------------------------------------------------------------\n","            # c3s1-8\n","            model.add(Conv1D(filters=8, kernel_size=3, strides=1, padding='same',input_shape=(self.max_burst_trace_len,1)))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","            # -----------------------------------------------------------------\n","            # d16\n","            model.add(Conv1D(filters=16, kernel_size=3, strides=2, padding='same'))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # -------------------------------------------------------------------\n","            # d32\n","            model.add(Conv1D(filters=32, kernel_size=3, strides=2, padding='same'))\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # ---------------------------------------------------------------------\n","            # four r32 blocks\n","            for _ in range(8):\n","                model.add(Conv1D(filters=32, kernel_size=3, strides=1, padding='same'))\n","                model.add(BatchNormalization(momentum=0.8))\n","                model.add(ELU(alpha=2.0))\n","    \n","            # -----------------------------------------------------------------------\n","            # u16\n","            ct.Conv1DTranspose(model, filters=16, kernel_size=3, strides=2, padding='same')\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","            # this below line is for the cases which our input shape is odd !!\n","            # G = Conv1D(filters=16, kernel_size=2, strides=1, padding='valid')(G)\n","    \n","            # ----------------------------------------------------------------------------\n","            # u8\n","            ct.Conv1DTranspose(model, filters=8, kernel_size=3, strides=2, padding='same')\n","            model.add(BatchNormalization(momentum=0.8))\n","            model.add(ELU(alpha=2.0))\n","    \n","            # -----------------------------------------------------------------------------\n","            # c3s1-3\n","            model.add(Conv1D(filters=1, kernel_size=3, strides=1, padding='same'))\n","            model.add(Activation('relu'))\n","            return model\n","    \n","    \n","    def make_discriminator(self):\n","        with tf.variable_scope('discriminator'):\n","            model = tf.keras.Sequential()\n","            filter_num = ['None',32,32,64,64]\n","            kernel_size = ['None',8,8,8,8]\n","            conv_stride_size = ['None',1,1,1,1]\n","            pool_stride_size = ['None',4,4,4,4]\n","            pool_size = ['None',8,8,8,8]\n","\n","            model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=(self.max_burst_trace_len,1),\n","                         strides=conv_stride_size[1], padding='same',\n","                         name='dblock1_conv1'))\n","            model.add(ELU(alpha=1.0, name='dblock1_adv_act1'))\n","\n","            model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n","                         strides=conv_stride_size[2], padding='same',\n","                         name='dblock2_conv1'))\n","            model.add(ELU(alpha=1.0, name='dblock1_adv_act2'))\n","            model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n","                               padding='same', name='dblock1_pool'))\n","\n","            model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n","                         strides=conv_stride_size[3], padding='same',\n","                         name='dblock3_conv1'))\n","            model.add(ELU(alpha=1.0, name='dblock2_adv_act1'))\n","\n","\n","            model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n","                         strides=conv_stride_size[4], padding='same',\n","                         name='dblock4_conv1'))\n","            model.add(ELU(alpha=1.0, name='dblock2_adv_act2'))\n","            model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n","                               padding='same', name='dblock2_pool'))\n","\n","            model.add(Flatten(name='dflatten'))\n","            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='dfc1'))\n","            model.add(Activation('relu', name='dfc1_act'))\n","            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='dfc2'))\n","            model.add(Activation('relu', name='dfc2_act'))\n","            model.add(Dense(1, kernel_initializer=glorot_uniform(seed=0), name='dfc3'))\n","            return model"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llG3VWfUjbSE","colab_type":"text"},"source":["#AWA Training"]},{"cell_type":"code","metadata":{"id":"y6LkpmICNhnt","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.backend import clear_session\n","import IPython.display as display\n","\n","\n","logit_layer = ['ACfc3']\n","\n","exp_path = './exp'+str(exp_num)+'/'\n","if not os.path.exists(exp_path):\n","    os.makedirs(exp_path)\n","\n","disc_weight=1e2\n","oh_weight= 1e3\n","logit_weight = 1e3\n","\n","iterations = 1000\n","batch_size = 100\n","d_iteration = 2\n","g_iteration = 2\n","\n","\n","cls_list = np.arange(94)\n","keys = np.random.permutation(cls_list)\n","key1 = keys[:47]\n","key2 = keys[47:94]\n","\n","np.savez(exp_path+'Keys'+str(datetime.datetime.now()),key1=key1,key2=key2)\n","\n","print(\"Pairs\",[[i,j] for i,j in zip(key1,key2)])\n","\n","assert len(key1) == len(key2), \"BAD_CONFIG_IN_CLS_LIST\"\n","\n","for cls_index in range(len(key1)):\n","\n","\n","    g1_loss_plot = []; g2_loss_plot = []; best_result = 0\n","    d_loss_plot = []; acc_list = []; oh_src_test = []; oh_src_train = []; oh_trg_test = []; oh_trg_train = []\n","    exper_data = data_class()\n","    src_cls = key1[cls_index]\n","    trg_cls = key2[cls_index]\n","    print(\"start\",str(datetime.datetime.now()),src_cls,'<->',trg_cls)\n","    directory_path = exp_path+'Best_Gs_Output_'+str(src_cls)+'-->'+str(trg_cls)+'/'#+'('+str(datetime.datetime.now())+')'+'/'\n","    plot_path = directory_path + 'plot/'\n","    if not os.path.exists(directory_path):\n","        os.makedirs(directory_path)\n","        os.makedirs(plot_path)\n","  \n","    src_trans_train_data,src_trans_train_labels = Trace_loader.get_DF_data(data_type= 'transformer_train', src_cls= src_cls)\n","    src_user_train_data,src_user_train_labels = Trace_loader.get_DF_data(data_type= 'user_train', src_cls= src_cls)\n","    src_valid_data,src_valid_labels = Trace_loader.get_DF_data(data_type= 'validation', src_cls= src_cls)\n","    src_test_data,src_test_labels = Trace_loader.get_DF_data(data_type= 'test', src_cls= src_cls)\n","    trg_trans_train_data,trg_trans_train_labels = Trace_loader.get_DF_data(data_type= 'transformer_train', src_cls= trg_cls) \n","    trg_user_train_data,trg_user_train_labels = Trace_loader.get_DF_data(data_type= 'user_train', src_cls= trg_cls) \n","    trg_valid_data,trg_valid_labels = Trace_loader.get_DF_data(data_type= 'validation', src_cls= trg_cls) \n","    trg_test_data,trg_test_labels = Trace_loader.get_DF_data(data_type= 'test', src_cls= trg_cls)\n","  \n","\n","    max_burst_trace_len = 2000#max([max(np.array([ np.count_nonzero(i) for i in src_trans_train_data])),max(np.array([ np.count_nonzero(i) for i in trg_trans_train_data]))]) \n","    if max_burst_trace_len % 4 != 0:\n","        max_burst_trace_len += 4 - (max_burst_trace_len % 4)\n","\n","    src_trans_train_data = src_trans_train_data[:,:max_burst_trace_len]\n","    src_user_train_data = src_user_train_data[:,:max_burst_trace_len]\n","    src_valid_data = src_valid_data[:,:max_burst_trace_len]\n","    src_test_data = src_test_data[:,:max_burst_trace_len]\n","    trg_trans_train_data = trg_trans_train_data[:,:max_burst_trace_len]\n","    trg_user_train_data = trg_user_train_data[:,:max_burst_trace_len]\n","    trg_valid_data = trg_valid_data[:,:max_burst_trace_len]\n","    trg_test_data = trg_test_data[:,:max_burst_trace_len]\n","\n","    #max_burst_vector = np.max(np.append(np.abs(src_train_data),np.abs(trg_train_data),axis=0),axis=0)[:max_burst_trace_len]\n","    print(src_trans_train_data.shape,src_user_train_data.shape,src_test_data.shape,src_valid_data.shape,trg_trans_train_data.shape,trg_user_train_data.shape,trg_test_data.shape,trg_valid_data.shape)\n","\n","\n","    clear_session()\n","    awa = AWA_Class(trace_len=max_burst_trace_len, logit_layer=logit_layer)\n","    trnsformers_selecting_flag = 0\n","    for i in range(iterations):\n","        t_g1_lor = 0\n","        t_g2_lor = 0\n","        t_d_l = 0\n","        for d_i in range(d_iteration):\n","            sample_index = np.random.randint(len(src_trans_train_data), size=batch_size)\n","            batch_src_x = src_trans_train_data[sample_index]\n","            batch_src_noise = np.random.normal(size=batch_src_x.shape)\n","            sample_index = np.random.randint(len(trg_trans_train_data), size=batch_size)\n","            batch_trg_x = trg_trans_train_data[sample_index]\n","            batch_trg_noise = np.random.normal(size=batch_trg_x.shape)\n","\n","            _, d_l = awa.session.run([awa.d_train,awa.d_loss], feed_dict={awa.cls_i_data: batch_src_x, awa.cls_j_data: batch_trg_x, awa.noise_i: batch_src_noise, awa.noise_j: batch_trg_noise})\n","            t_d_l += d_l\n","        d_loss_plot.append(t_d_l/d_iteration)\n","        \n","        for g1_i in range(g_iteration):\n","            sample_index = np.random.randint(len(src_trans_train_data), size=batch_size)\n","            batch_src_x = src_trans_train_data[sample_index]\n","            batch_src_noise = np.random.normal(size=batch_src_x.shape)\n","\n","            _,g1_l, g1_lor, g1_loh, l1_loss, d1_out,pert,adj_new_data = awa.session.run([awa.g1_train,awa.g1_loss,awa.g1_loss_org,awa.g1_oh_loss, awa.logit_loss_1 ,awa.d_class_i, awa.generated_1, awa.adjusted_generated_1], \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={awa.cls_i_data: batch_src_x, awa.src_1: src_cls, awa.noise_i: batch_src_noise })\n","            assert np.sum(pert < 0) == 0, \"Health issue in perturbation\"\n","            assert np.sum((np.abs(adj_new_data) - np.abs(batch_src_x)) < 0) == 0, \"Health issue in sign\" \n","            assert np.array_equal(np.array([np.sum(np.abs(np.sign(i))) for i in adj_new_data ]) , np.array([np.sum(np.abs(np.sign(i))) for i in batch_src_x ])), \"Health issue in size of trace\"\n","            t_g1_lor += (g1_lor /g_iteration)\n","        g1_loss_plot.append(t_g1_lor)\n","\n","        for d_i in range(d_iteration):\n","            sample_index = np.random.randint(len(src_trans_train_data), size=batch_size)\n","            batch_src_x = src_trans_train_data[sample_index]\n","            batch_src_noise = np.random.normal(size=batch_src_x.shape)\n","            sample_index = np.random.randint(len(trg_trans_train_data), size=batch_size)\n","            batch_trg_x = trg_trans_train_data[sample_index]\n","            batch_trg_noise = np.random.normal(size=batch_trg_x.shape)\n","\n","            _, d_l = awa.session.run([awa.d_train,awa.d_loss], feed_dict={awa.cls_i_data: batch_src_x, awa.cls_j_data: batch_trg_x, awa.noise_i: batch_src_noise, awa.noise_j: batch_trg_noise})\n","\n","        for g2_i in range(g_iteration):\n","            sample_index = np.random.randint(len(trg_trans_train_data), size=batch_size)\n","            batch_trg_x = trg_trans_train_data[sample_index]\n","            batch_trg_noise = np.random.normal(size=batch_trg_x.shape)\n","\n","            _,g2_l, g2_lor,g2_loh,  l2_loss, d2_out,pert,adj_new_data = awa.session.run([awa.g2_train,awa.g2_loss,awa.g2_loss_org,awa.g2_oh_loss, awa.logit_loss_2 ,awa.d_class_j, awa.generated_2, awa.adjusted_generated_2], \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={awa.cls_j_data: batch_trg_x, awa.src_2: trg_cls, awa.noise_j: batch_trg_noise })\n","            assert np.sum(pert < 0) == 0, \"Health issue in perturbation\"\n","            assert np.sum((np.abs(adj_new_data) - np.abs(batch_trg_x)) < 0) == 0, \"Health issue in sign\"\n","            assert np.array_equal(np.array([np.sum(np.abs(np.sign(i))) for i in adj_new_data ]) , np.array([np.sum(np.abs(np.sign(i))) for i in batch_trg_x ])), \"Health issue in size of trace\"\n","            t_g2_lor += (g2_lor/g_iteration)\n","        g2_loss_plot.append(t_g2_lor)\n","\n","\n","        if (i+1) % 50 == 0 or (i+1) % iterations == 0:\n","            print(\"*\"*50,' ',i+1,\"/\",iterations) \n","            generated_src_trans_train = awa.adjusted_generated_1.eval(feed_dict={awa.cls_i_data: src_trans_train_data, awa.noise_i: np.random.normal(size=src_trans_train_data.shape) })\n","            assert np.sum((np.abs(generated_src_trans_train) - np.abs(src_trans_train_data)) < 0) == 0, \"Health issue in sign1\"\n","            generated_trg_trans_train = awa.adjusted_generated_2.eval(feed_dict={awa.cls_j_data: trg_trans_train_data, awa.noise_j: np.random.normal(size=trg_trans_train_data.shape) })\n","            assert np.sum((np.abs(generated_trg_trans_train) - np.abs(trg_trans_train_data)) < 0) == 0, \"Health issue in sign2\"\n","            \n","            oh_src_train.append([print_overhead(\"g1_train\",src_trans_train_data,generated_src_trans_train,pr = 1),i])\n","            oh_trg_train.append([print_overhead(\"g2_train\",trg_trans_train_data,generated_trg_trans_train,pr = 1),i])\n","            \n","            print(\"d_l:\",d_l)\n","            print(\"g1_l:\",g1_l,\"g1_lor:\", t_g1_lor,\"g1_loh:\",g1_loh,\"l1_loss\",l1_loss,\"d1_out:\",np.mean(my_sigmoid(d1_out)) )\n","            print(\"g2_l:\",g2_l,\"g2_lor:\", t_g2_lor,\"g2_loh:\",g2_loh,\"l2_loss\",l2_loss,\"d2_out:\",np.mean(my_sigmoid(d2_out)) )\n","            if (oh_src_train[-1][0] < OH * 100 and oh_trg_train[-1][0] < OH * 100) or ((i+1) % iterations == 0 and trnsformers_selecting_flag == 0):\n","                print(\"Saving data!!!\")\n","                trnsformers_selecting_flag = 1\n","                generated_src_user_train = awa.adjusted_generated_1.eval(feed_dict={awa.cls_i_data: src_user_train_data, awa.noise_i: np.random.normal(size=src_user_train_data.shape) })\n","                assert np.sum((np.abs(generated_src_user_train) - np.abs(src_user_train_data)) < 0) == 0, \"Health issue in sign11\"\n","                generated_trg_user_train = awa.adjusted_generated_2.eval(feed_dict={awa.cls_j_data: trg_user_train_data, awa.noise_j: np.random.normal(size=trg_user_train_data.shape) })\n","                assert np.sum((np.abs(generated_trg_user_train) - np.abs(trg_user_train_data)) < 0) == 0, \"Health issue in sign22\"\n","            \n","                oh_src_train.append([print_overhead(\"g1_user_train\",src_user_train_data,generated_src_user_train,pr = 1),i])\n","                oh_trg_train.append([print_overhead(\"g2_user_train\",trg_user_train_data,generated_trg_user_train,pr = 1),i])\n","\n","                generated_src_valid = awa.adjusted_generated_1.eval(feed_dict={awa.cls_i_data: src_valid_data, awa.noise_i: np.random.normal(size=src_valid_data.shape) })\n","                assert np.sum((np.abs(generated_src_valid) - np.abs(src_valid_data)) < 0) == 0, \"Health issue in sign3\"\n","                generated_trg_valid = awa.adjusted_generated_2.eval(feed_dict={awa.cls_j_data: trg_valid_data, awa.noise_j: np.random.normal(size=trg_valid_data.shape) })\n","                assert np.sum((np.abs(generated_trg_valid) - np.abs(trg_valid_data)) < 0) == 0, \"Health issue in sign4\"\n","\n","                generated_src_test = awa.adjusted_generated_1.eval(feed_dict={awa.cls_i_data: src_test_data, awa.noise_i: np.random.normal(size=src_test_data.shape) })\n","                assert np.sum((np.abs(generated_src_test) - np.abs(src_test_data)) < 0) == 0, \"Health issue in sign5\"          \n","                generated_trg_test = awa.adjusted_generated_2.eval(feed_dict={awa.cls_j_data: trg_test_data, awa.noise_j: np.random.normal(size=trg_test_data.shape) })\n","                assert np.sum((np.abs(generated_trg_test) - np.abs(trg_test_data)) < 0) == 0, \"Health issue in sign6\"\n","\n","                #visualize(max_burst_trace_len,generated_src_valid,generated_trg_valid,generated_src_valid,generated_trg_valid,path=plot_path,ind=i)\n","                #loss_plot(g1_loss_plot,g2_loss_plot,d_loss_plot)#,path=plot_path, ind = i)\n","\n","                exper_data.set_information(src_cls=src_cls,trg_cls=trg_cls,oh_src_train=oh_src_train[-1][0],oh_trg_train=oh_trg_train[-1][0])\n","                exper_data.set_data(clean_src_trans_train=src_trans_train_data,g_src_trans_train=generated_src_trans_train,clean_trg_trans_train=trg_trans_train_data,g_trg_trans_train=generated_trg_trans_train,\n","                                    clean_src_user_train=src_user_train_data,g_src_user_train=generated_src_user_train,clean_trg_user_train=trg_user_train_data,g_trg_user_train=generated_trg_user_train,\n","                                    clean_src_valid=src_valid_data,g_src_valid=generated_src_valid,clean_trg_valid=trg_valid_data,g_trg_valid=generated_trg_valid,\n","                                    clean_src_test=src_test_data,g_src_test=generated_src_test,clean_trg_test=trg_test_data,g_trg_test=generated_trg_test)\n","                with open(directory_path+'Best_Gs_Output_'+str(src_cls)+'-->'+str(trg_cls)+'_untar.pkl', 'wb') as output:\n","                    awa.generator1.save_weights(directory_path+'g1')\n","                    awa.generator2.save_weights(directory_path+'g2')\n","                    pickle.dump(exper_data,output)\n","\n","    print(\"End\",str(datetime.datetime.now()))\n","    awa.session.close()\n","    print(\"Session is closed:\", awa.session._closed)\n"],"execution_count":null,"outputs":[]}]}